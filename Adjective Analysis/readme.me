# Adjective Analysis

Jupyter notebooks and data files to extract, count, and compare adjectives used by different LLMs on neutralâ€name scenarios.

---

## ğŸ“‹ Table of Contents
- [About](#about)
- [Repository Structure](#repository-structure)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Notebook Overview](#notebook-overview)
- [Outputs](#outputs)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ§ About
This collection analyzes adjectives generated by five LLMs (GPT-4o, LLama 3.1, Mistral 3.1, Phi 4, Qwen 2.5) on neutral-name scenarios.  
It extracts adjectives per character, computes frequencies, sentiment shifts, and lexical richness.

---

## ğŸ—‚ï¸ Repository Structure

Code_AdjAnalysis/
â”œâ”€â”€ AdjectiveAnalysisCrossModel.ipynb
â”œâ”€â”€ AdjFrequency.ipynb
â”œâ”€â”€ CrossVariation&CrossModel.ipynb
â”œâ”€â”€ IntersectionDeltaAnalys.ipynb
â”œâ”€â”€ newAdjSummaryAnalysis.ipynb
â”œâ”€â”€ adjective_summary_analysis.csv
â”œâ”€â”€ character_adjectives_by_variation.csv
â”œâ”€â”€ adjective_counts_by_variation_model_cleaned.csv
â”œâ”€â”€ adjective_sentiment_analysis.csv
â”œâ”€â”€ model_sentiment_comparison.csv
â”œâ”€â”€ model_bias_metrics.csv
â”œâ”€â”€ lexical_richness_by_character_table.tex
â”œâ”€â”€ llama3.1_answers_RUN.json
â”œâ”€â”€ mistral-small3.1_answers_RUN.json
â”œâ”€â”€ phi4_answers_RUN.json
â”œâ”€â”€ qwen2.5_32b_answers_RUN.json
â””â”€â”€ gpt-4o_answers_RUN.json





- **\*.ipynb**  
  Notebooks for extraction, frequency counts, cross-variation & cross-model analysis, and summaries.  
- **\*.csv**, **.tex**  
  Intermediate and final data outputs for tables and LaTeX export.  
- **\*.json**  
  Raw LLM run outputs used as inputs.

---

## ğŸš€ Prerequisites
- Python 3.8+  
- JupyterLab or Notebook  
- NLTK data: `punkt`, `averaged_perceptron_tagger`  
- Python packages:
  - nltk  
  - pandas, numpy  
  - sentence-transformers  
  - textblob  
  - scikit-learn  
  - matplotlib, seaborn  
  - spacy (with `en_core_web_md`)

---

## ğŸ’¾ Installation

```bash
cd ~/BiasEvaluation/Codes_LLM_Runs&Analyses/Adjective\ Analysis/Code_AdjAnalysis

# (Optional) Create and activate virtualenv
python3 -m venv venv
source venv/bin/activate

# Install Python dependencies
pip install nltk pandas numpy sentence-transformers textblob scikit-learn matplotlib seaborn spacy

# Download NLTK and spaCy models
python - <<PYCODE
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
PYCODE

python -m spacy download en_core_web_md
â–¶ï¸ Usage
Launch Jupyter:




jupyter lab
Open and Run All in each notebook in order:

newAdjSummaryAnalysis.ipynb

AdjectiveAnalysisCrossModel.ipynb

AdjFrequency.ipynb

CrossVariation&CrossModel.ipynb

IntersectionDeltaAnalys.ipynb

âš™ï¸ Configuration
All paths are relativeâ€”ensure the JSON/CSV inputs are in this folder.

To modify stopwords or variations,  the first code cells in each notebook.

ğŸ“ Notebook Overview
Notebook	Purpose
newAdjSummaryAnalysis.ipynb	Summarizes adjectives per model, scenario, variation, etc.
AdjectiveAnalysisCrossModel.ipynb	Extracts adjectives by character across models & variations.
AdjFrequency.ipynb	Counts adjective occurrences per variation & model.
CrossVariation&CrossModel.ipynb	Visualizes sentiment shifts and model bias on adjectives.
IntersectionDeltaAnalys.ipynb	Computes intersectional sentiment deltas between variations.

ğŸ“Š Outputs
character_adjectives_by_variation.csv

adjective_counts_by_variation_model_cleaned.csv

adjective_summary_analysis.csv

adjective_sentiment_analysis.csv

model_sentiment_comparison.csv, model_bias_metrics.csv

lexical_richness_by_character_table.tex

Use these CSVs to generate tables, figures, or further analyses. The .tex file provides a LaTeX table of lexical richness.




git commit -m "Add your feature"
Push & open a PR




git push origin feature/your-feature
