## ðŸš€ Getting Started
Welcome to the **BiasEvaluation** repository! This project contains a suite of analyses on neutral-name scenarios using various large language models (LLMs). The work is organized into four main tasks, each in its own subfolder. Click the links below to jump directly to each taskâ€™s README.

---

## ðŸ“‚ Tasks

### 1. Running LLMs
Load scenarios from JSON, generate variations, run five questions through multiple LLMs, and collect answers.  
[See detailed instructions â†’](Codes_LLM_Runs&Analyses/Running_LLMs/readme.me)

---

### 2. Scenario Statistics
Compute descriptive statistics (tokens, readability, sentiment, similarity) across all scenarios and visualize distributions.  
[See detailed instructions â†’](Codes_LLM_Runs&Analyses/Scenarios/readme.me)

---

### 3. Counterfactual Analysis
Perform counterfactual comparisons on model outputs: semantic similarity and sentiment shifts between scenario variations for each LLM, plus cross-model summaries.  
[See detailed instructions â†’](Codes_LLM_Runs&Analyses/Counterfactual%20Analysis/readme.me)

---

### 4. Adjective Analysis
Extract and quantify adjectives used by each model for each character and variation; analyze frequency, sentiment bias, and lexical richness.  
[See detailed instructions â†’](Codes_LLM_Runs&Analyses/Adjective%20Analysis/readme.me)
