# Adjective Analysis

Jupyter notebooks and data files to extract, count, and compare adjectives used by different LLMs on neutral‐name scenarios.

---

## 📋 Table of Contents
- [About](#about)
- [Repository Structure](#repository-structure)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Notebook Overview](#notebook-overview)
- [Outputs](#outputs)
- [Contributing](#contributing)
- [License](#license)

---

## 🧐 About
This collection analyzes adjectives generated by five LLMs (GPT-4o, LLama 3.1, Mistral 3.1, Phi 4, Qwen 2.5) on neutral-name scenarios.  
It extracts adjectives per character, computes frequencies, sentiment shifts, and lexical richness.

---

## 🗂️ Repository Structure

Code_AdjAnalysis/
├── AdjectiveAnalysisCrossModel.ipynb
├── AdjFrequency.ipynb
├── CrossVariation&CrossModel.ipynb
├── IntersectionDeltaAnalys.ipynb
├── newAdjSummaryAnalysis.ipynb
├── adjective_summary_analysis.csv
├── character_adjectives_by_variation.csv
├── adjective_counts_by_variation_model_cleaned.csv
├── adjective_sentiment_analysis.csv
├── model_sentiment_comparison.csv
├── model_bias_metrics.csv
├── lexical_richness_by_character_table.tex
├── llama3.1_answers_RUN.json
├── mistral-small3.1_answers_RUN.json
├── phi4_answers_RUN.json
├── qwen2.5_32b_answers_RUN.json
└── gpt-4o_answers_RUN.json





- **\*.ipynb**  
  Notebooks for extraction, frequency counts, cross-variation & cross-model analysis, and summaries.  
- **\*.csv**, **.tex**  
  Intermediate and final data outputs for tables and LaTeX export.  
- **\*.json**  
  Raw LLM run outputs used as inputs.

---

## 🚀 Prerequisites
- Python 3.8+  
- JupyterLab or Notebook  
- NLTK data: `punkt`, `averaged_perceptron_tagger`  
- Python packages:
  - nltk  
  - pandas, numpy  
  - sentence-transformers  
  - textblob  
  - scikit-learn  
  - matplotlib, seaborn  
  - spacy (with `en_core_web_md`)

---

## 💾 Installation

```bash
cd ~/BiasEvaluation/Codes_LLM_Runs&Analyses/Adjective\ Analysis/Code_AdjAnalysis

# (Optional) Create and activate virtualenv
python3 -m venv venv
source venv/bin/activate

# Install Python dependencies
pip install nltk pandas numpy sentence-transformers textblob scikit-learn matplotlib seaborn spacy

# Download NLTK and spaCy models
python - <<PYCODE
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
PYCODE

python -m spacy download en_core_web_md
▶️ Usage
Launch Jupyter:




jupyter lab
Open and Run All in each notebook in order:

newAdjSummaryAnalysis.ipynb

AdjectiveAnalysisCrossModel.ipynb

AdjFrequency.ipynb

CrossVariation&CrossModel.ipynb

IntersectionDeltaAnalys.ipynb

⚙️ Configuration
All paths are relative—ensure the JSON/CSV inputs are in this folder.

To modify stopwords or variations,  the first code cells in each notebook.

📝 Notebook Overview
Notebook	Purpose
newAdjSummaryAnalysis.ipynb	Summarizes adjectives per model, scenario, variation, etc.
AdjectiveAnalysisCrossModel.ipynb	Extracts adjectives by character across models & variations.
AdjFrequency.ipynb	Counts adjective occurrences per variation & model.
CrossVariation&CrossModel.ipynb	Visualizes sentiment shifts and model bias on adjectives.
IntersectionDeltaAnalys.ipynb	Computes intersectional sentiment deltas between variations.

📊 Outputs
character_adjectives_by_variation.csv

adjective_counts_by_variation_model_cleaned.csv

adjective_summary_analysis.csv

adjective_sentiment_analysis.csv

model_sentiment_comparison.csv, model_bias_metrics.csv

lexical_richness_by_character_table.tex

Use these CSVs to generate tables, figures, or further analyses. The .tex file provides a LaTeX table of lexical richness.




git commit -m "Add your feature"
Push & open a PR




git push origin feature/your-feature
